{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local learning rules to attenuate forgetting in neural networks\n",
    "This demo will guide you through the basic steps of the algorithm used in our paper. We are thereby especially aiming to reproduce the key figure 3b, which represents the forgetting of patterns in continuous learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Framework\n",
    "We first set up our framework. We load the parameters needed for our plots, load our classes and libraries and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define the parameters for our plots such as label sizes\n",
    "%run -i 'define_plotting_framework.py'\n",
    "\n",
    "# Create to_triangular(), from_triangular(), dice_coefficient() and evaluate_stability()\n",
    "%run -i 'define_helper_functions.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters\n",
    "We now define the hyperparameters. Their function is given in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ETA = 0.1  # learning rate\n",
    "SPARSITY = 0.5  # number of zeros: SPARSITY = 0.1 means 10% ones and 90% zeros\n",
    "IMAGE_SIZE = 10  # the size of our pattern will be (IMAGE_SIZE x IMAGE_SIZE)\n",
    "TRIALS = 10  # number of trials over which the results will be averaged\n",
    "\n",
    "epochs_patterns_presented = 5 # number of epochs a single pattern is presented\n",
    "n_stored_patterns = 0 # number of instantaneously stored patterns in our network at the beginning of learning\n",
    "n_new_patterns = 10 # number of continuously stored patterns in our network\n",
    "eval_epochs = 10 # checking stability of patterns after eval_epochs iterations\n",
    "\n",
    "n_tot_patterns = n_stored_patterns + n_new_patterns\n",
    "NTRAIN = epochs_patterns_presented * n_new_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Learning\n",
    "Setting these parameters to True will lead to an evaluation of the respective method. False will discard the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set methods to True is they should be evaluated, to False for discarding\n",
    "USE_HOPFIELD = True # classical Hebbian Learning rule for Hopfield networks\n",
    "USE_EXPONENTIAL = True # exponential rule\n",
    "USE_EXPONENTIAL_THRESHOLD = True # exponential rule with threshold\n",
    "USE_BAYES = True # bayesian with first order polynomial\n",
    "USE_BAYES_POW = True # bayesian with second order polynomial\n",
    "USE_BAYES_THRESHOLD = True # bayesian with threshold\n",
    "USE_W_THRESHOLD = True # weight threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize network\n",
    "We define our network, the patterns and the mean-free patterns. We then initialize the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparing patterns\n",
    "solver = solverClass()\n",
    "patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, n_tot_patterns)\n",
    "netFisher = hopfieldNet(IMAGE_SIZE, ETA, SPARSITY)\n",
    "original_patterns = copy.deepcopy(patterns)\n",
    "patterns = patterns - SPARSITY\n",
    "\n",
    "# creating an empty weight matrix. Will be filled in the next lines\n",
    "p = np.zeros(shape=(IMAGE_SIZE**2, IMAGE_SIZE**2))    \n",
    "\n",
    "if n_stored_patterns>0: # initialize weight matrix with n_stored_patterns\n",
    "    for i in range(n_stored_patterns):\n",
    "        p += np.outer(patterns[:, i], patterns[:, i])\n",
    "        netFisher.append_pattern(patterns[:, i], NTRAIN)\n",
    "    w1 = p/n_stored_patterns\n",
    "else: # initialize the network with other patterns that will not be considered for the evaluation\n",
    "    ns = 20\n",
    "    pre_patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, ns) - SPARSITY\n",
    "    for i in range(ns):\n",
    "        p += np.outer(pre_patterns[:, i], pre_patterns[:, i])\n",
    "    w1 = p/ns/1\n",
    "netFisher.set_weights(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation functions\n",
    "We set up the functions needed for calculating the Dice-coefficient. We evaluate all patterns and store their Dice-coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the function that w |-> Fisher(w)\n",
    "# originally (Michael D's code), we used identity\n",
    "def fisher_function(w):\n",
    "    return -w**2\n",
    "\n",
    "def compute_dice(ETA, c, t, func, plot=False):#, t = 0.0):\n",
    "    wF = np.copy(w1)\n",
    "    DICE = -np.ones(shape=(NTRAIN, n_tot_patterns))\n",
    "    N_PRE = 0\n",
    "    NTRAIN_PRE = epochs_patterns_presented * N_PRE  # number of epochs\n",
    "    pre_patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, N_PRE)\n",
    "\n",
    "    all_patterns = np.hstack((pre_patterns - SPARSITY,patterns))\n",
    "    w_mean = np.zeros(NTRAIN+NTRAIN_PRE)\n",
    "    for epoch in range(NTRAIN+NTRAIN_PRE):\n",
    "        id_pattern_taught = n_stored_patterns+epoch//epochs_patterns_presented\n",
    "        pattern_taught = all_patterns[:, id_pattern_taught]\n",
    "\n",
    "        z = (np.outer(pattern_taught, pattern_taught) - wF)\n",
    "\n",
    "        F_ = fisher_function(wF)\n",
    "        perturbation_vector = func(F_, c, np.abs(z), t) * z * ETA\n",
    "        wF = wF + perturbation_vector\n",
    "        netFisher.set_weights(wF)\n",
    "        w_mean[epoch] = np.mean(wF)\n",
    "\n",
    "        # checking stability of patterns after eval_epochs iterations\n",
    "        if epoch>=NTRAIN_PRE:\n",
    "            DICE[epoch-NTRAIN_PRE] = evaluate_stability(  # old patterns\n",
    "                netFisher, original_patterns.T[:n_tot_patterns], average=False)\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.hist(wF.ravel())\n",
    "    return DICE\n",
    "\n",
    "def compute_dice_error(par,args):\n",
    "    if len(par)==3:\n",
    "        ETA, c, t = par\n",
    "    elif len(par)==2:\n",
    "        ETA, c, t = par[0], par[1], 0\n",
    "    else:\n",
    "        ETA, c, t = par[0], 0, 0\n",
    "    return -np.sum(compute_dice(ETA, c, t, args)>0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Parameters\n",
    "We choose the learning rule(s) to be used and evaluate and optimize their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.optimize import minimize\n",
    "from scipy import optimize\n",
    "\n",
    "# These are the possible functions we can use\n",
    "expo = lambda wF, c, z, t: np.exp(-c*np.abs(wF))\n",
    "bayes = lambda wF, c, z, t: c/(c+np.abs(wF))\n",
    "bayes_pow = lambda wF, c, z, t: c/(c+np.abs(wF)*(1-np.abs(wF)))\n",
    "bayes_thres = lambda wF, c, z, t: c/(c+np.abs(wF))*(z>t)\n",
    "w_thres  = lambda wF, c, z, t: wF<c\n",
    "hopfield  = lambda wF, c, z, t: np.ones(wF.shape)\n",
    "expo_thres = lambda wF, c, z, t: np.exp(-c*np.abs(wF))*(z>t)\n",
    "\n",
    "# use the scipy inbuilt function optimize.fmin() to optimize the parameters of our learning rule\n",
    "%run -i 'optimize.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Network\n",
    "We are now going to run our network with the optimized set of parameter obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary that will store the data\n",
    "%run -i 'create_data_dictionary.py'\n",
    "\n",
    "# loop over trials over which will later on be averaged\n",
    "plot = True\n",
    "for i in range(TRIALS):\n",
    "\n",
    "    patterns = solver.create_patterns(SPARSITY, IMAGE_SIZE, n_tot_patterns)\n",
    "    original_patterns = copy.deepcopy(patterns)\n",
    "    patterns = patterns - SPARSITY\n",
    "    \n",
    "    num_iteration = 0\n",
    "    for key in data.keys():\n",
    "        \n",
    "        c = parameters[order_dict[key]]\n",
    "\n",
    "        if key == 'hopfield':\n",
    "            data['hopfield'].append(compute_dice(c[0], 0, 0, hopfield, plot))\n",
    "\n",
    "        if key == 'exponential':\n",
    "            data['exponential'].append(compute_dice(c[0], c[1], 0, expo, plot))\n",
    "\n",
    "        if key == 'exponenital_thr':\n",
    "            data['exponenital_thr'].append(compute_dice(c[0], c[1], c[2], expo_thres, plot))\n",
    "\n",
    "        if key == 'bayes':\n",
    "            data['bayes'].append(compute_dice(c[0], c[1], 0, bayes, plot))\n",
    "\n",
    "        if key == 'bayes_pow':\n",
    "            data['bayes_pow'].append(compute_dice(c[0], c[1], 0, bayes_pow, plot))\n",
    "\n",
    "        if key == 'bayes_thr':\n",
    "            data['bayes_thr'].append(compute_dice(c[0], c[1], c[2], bayes_thres, plot))\n",
    "            \n",
    "        if key == 'w_thr':\n",
    "            data['w_thr'].append(compute_dice(c[0], c[1], 0, w_thres, plot))\n",
    "            \n",
    "        num_iteration += 1\n",
    "    plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 3*num_evaluations))\n",
    "plot_number = 1\n",
    "\n",
    "for key in data.keys():\n",
    "        \n",
    "    ax = plt.subplot(num_evaluations, 1, plot_number)\n",
    "    \n",
    "    ax.set_title(key)\n",
    "\n",
    "    center = np.mean(data[key], axis=0)\n",
    "    \n",
    "    # print(np.mean(center))\n",
    "    n,m = center.shape\n",
    "    im = ax.imshow(center.T,cmap=plt.cm.summer,aspect=0.2*n/m,vmin=0,vmax=1)\n",
    "    \n",
    "    if plot_number == 1:\n",
    "        #plt.colorbar(ax)\n",
    "        #ax.colorbar(im)\n",
    "        #cbaxes = ax.add_axes([0.1, 0.1, 0.03, 0.8])\n",
    "        ax.set_ylabel('Pattern')\n",
    "\n",
    "    if plot_number == num_evaluations:\n",
    "        ax.set_xlabel('Time (epochs)')\n",
    "    plot_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
